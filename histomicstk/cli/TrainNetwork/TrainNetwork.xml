<?xml version="1.0" encoding="UTF-8"?>
<executable>
  <category>HistomicsTK</category>
  <title>Train Segmentation Network</title>
  <description>Trains a neural network to segment structures from whole-slide image</description>
  <version>0.1.0</version>
  <documentation-url>https://github.com/SarderLab/deeplab-WSI</documentation-url>
  <license>Apache 2.0</license>
  <contributor>Brendon Lutnick (SUNY Buffalo)</contributor>
  <acknowledgements>This work is part of efforts in digital pathology by the Sarder Lab: SUNY Buffalo.</acknowledgements>
  <parameters>
    <label>IO</label>
    <description>Input/output parameters</description>
    <directory>
      <name>inputFolder</name>
      <label>Training Data Folder</label>
      <description>Select the folder containing the slides to be used for training</description>
      <channel>input</channel>
      <index>0</index>
    </directory>
    <file fileExtensions=".zip" reference="modelfiles">
      <name>output_model</name>
      <label>Output Model Name</label>
      <description>Select the name of the output model file produced. By default this will be saved in your Private folder.</description>
      <channel>output</channel>
      <index>1</index>
    </file>
    <string-vector>
      <name>classes</name>
      <longflag>classes</longflag>
      <label>Training layers</label>
      <description>A comma spearated list of the annotation layer names used for training. The name needs to match exactly. Layers will be built up from the annotations in order, sub-compartments should come after their parent structures.</description>
      <default>name1,name2,name3,etc</default>
    </string-vector>
    <file fileExtensions=".zip" defaultNameMatch="^model.*.zip$" defaultPathMatch="^\/collection\/Segmentation models\/ImageNet model\/">
      <name>inputModelFile</name>
      <label>Input Model File - (zip file)</label>
      <channel>input</channel>
      <index>1</index>
      <description>A zip file containing Tensorflow model-files and metadata for the deeplab segmentation network. This is used as a starting point for training using transfer learning.</description>
    </file>
  </parameters>
  <parameters advanced="true">
    <label>WSI Training Parameters</label>
    <description>Parameters for training a whole-slide image (WSI) segmentation algorythm</description>
    <integer>
      <name>patch_size</name>
      <label>Training tile size</label>
      <description>Tile size for randomly cropped WSI regions (pixels)</description>
      <longflag>patch_size</longflag>
      <default>400</default>
    </integer>
    <integer>
      <name>steps</name>
      <label>Training steps</label>
      <description>The number of steps used for network training. The network will see [steps * batch size] image patches during training</description>
      <longflag>steps</longflag>
      <default>5000</default>
    </integer>
    <integer>
      <name>global_step</name>
      <label>Global step</label>
      <description>The global step to use at the start of training. This is useful to continue training a previously trained network.</description>
      <longflag>global_step</longflag>
      <default>0</default>
    </integer>
    <integer>
      <name>batch_size</name>
      <label>Training batch size</label>
      <description>The batch size for training on WSI patches</description>
      <longflag>batch_size</longflag>
      <default>2</default>
    </integer>
    <string>
      <name>ignore_label</name>
      <longflag>ignore_label</longflag>
      <label>Ignore class</label>
      <description>The name of an annotation layer that will be ignored durring training. This allows parts of the WSI to be ignored for training.</description>
      <default>ignore</default>
    </string>
    <string>
      <name>GPU</name>
      <longflag>gpu</longflag>
      <label>GPU IDs</label>
      <description>A comma separated list of the GPU IDs that will be made avalable for training</description>
      <default>0</default>
    </string>
    <integer>
      <name>num_clones</name>
      <label>GPU number of clones</label>
      <description>The number of GPUs used for training. (only change is multiple are available)</description>
      <longflag>num_clones</longflag>
      <default>1</default>
    </integer>
    <integer-vector>
      <name>WSI_downsample</name>
      <label>Training patch scale rates</label>
      <description>The downsampling (w.r.t full slide resolution) applied to the WSI patches durring extraction - durring training. The values passed will be randomly used durring the training. We find that prerformance improves if the network is shown multiple different WSI scales at training time.</description>
      <longflag>WSI_downsample</longflag>
      <default>1,2,3,4</default>
    </integer-vector>
    <double>
      <name>learning_rate</name>
      <label>Network learning rate</label>
      <description>The base learning rate used by the network. This will decay over the course of training.</description>
      <longflag>learning_rate</longflag>
      <default>0.0005</default>
    </double>
    <double>
      <name>learning_rate_start</name>
      <label>Slow start learning rate</label>
      <description>The initial learning rate used by the network. This should be set lower than the base learning rate to avoid shocking the model initially.</description>
      <longflag>learning_rate_start</longflag>
      <default>0.00001</default>
    </double>
    <integer>
      <name>slow_start_step</name>
      <label>Slow start steps</label>
      <description>The number of steps the network should be trained using the slow start learning rate.</description>
      <longflag>slow_start_step</longflag>
      <default>1000</default>
    </integer>
    <double>
      <name>end_learning_rate</name>
      <label>End learning rate</label>
      <description>The end learning rate reached by the network. The learning rate will decay to this value.</description>
      <longflag>end_learning_rate</longflag>
      <default>0.0</default>
    </double>
    <double>
      <name>learning_power</name>
      <label>Learning power</label>
      <description>The power for polynomial learning rate decay.</description>
      <longflag>learning_power</longflag>
      <default>0.9</default>
    </double>
    <integer>
      <name>decay_steps</name>
      <label>Learning rate decay steps</label>
      <description>The number of steps before the learning rate decays to the end learning rate. A value of 0 will decay by the end of training.</description>
      <longflag>decay_steps</longflag>
      <default>0</default>
    </integer>
    <double>
      <name>augment</name>
      <label>Patch augmentation percent</label>
      <description>The percentage (as a decimal) of training patches that will be augmented by random color shifting and piecewise-affine transformations. Please enter a number between [0-1], a value of 0 will have no augmentation. Note - augmentation slows down network training and should be used sparingly!</description>
      <longflag>augment</longflag>
      <default>0.01</default>
    </double>
    <boolean>
      <name>batch_norm</name>
      <label>Fine tune batch norm layers</label>
      <longflag>batch_norm</longflag>
      <description>Fine tune the batch normalization layers - this should only be set to True if the batch size is sufficiently large. The official Deeplab implementation recommends a batch size > 12.</description>
      <default>false</default>
    </boolean>
    <boolean>
      <name>init_last_layer</name>
      <label>Use previous classifier.</label>
      <longflag>init_last_layer</longflag>
      <description>Use the pervious networks classification layer. An option to initialze the new model using the parameters from the previous models last layer (classification layer). This can only be used if them model contains the same number of layers as the previous model.</description>
      <default>false</default>
    </boolean>
    <double>
      <name>last_layer_gradient_multiplier</name>
      <label>Last layer gradient multiplier</label>
      <description>For fine tuning, the gradients of the last (classification) layer can be boosted by a multiplier. Generally this is set to 10.0, or it can be set to 1.0 for no boosting.</description>
      <longflag>last_layer_gradient_multiplier</longflag>
      <default>10.0</default>
    </double>
    <boolean>
      <name>last_layers_contain_logits_only</name>
      <label>Last layer includes logits only</label>
      <longflag>last_layers_contain_logits_only</longflag>
      <description>Boost the graients of only the logits. If not selected the gradients of decoder, ASPP, and pooling layers will also be boosted.</description>
      <default>false</default>
    </boolean>
    <boolean>
      <name>upsample_logits</name>
      <label>Upsample logits</label>
      <longflag>upsample_logits</longflag>
      <description>If this is selected the logits (probablilities) of the network will be upsampled before the loss is calcluated. This increases the memory useage of the network.</description>
      <default>true</default>
    </boolean>
  </parameters>
  <parameters advanced="true">
    <label>Girder API URL and Key</label>
    <description>A Girder API URL and token for Girder client</description>
    <string>
      <name>girderApiUrl</name>
      <longflag>api-url</longflag>
      <label>Girder API URL</label>
      <description>A Girder API URL (e.g., https://girder.example.com:443/api/v1)</description>
      <default></default>
    </string>
    <string>
      <name>girderToken</name>
      <longflag>token</longflag>
      <label>Girder API Token</label>
      <description>A Girder token</description>
      <default></default>
    </string>
  </parameters>
</executable>
